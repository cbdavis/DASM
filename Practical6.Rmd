---
title: "Practical 6"
author: "Chris Davis"
date: "October 10, 2016"
output:
  html_document:
    number_sections: yes
    toc: yes
---

For this practical, we will need the `GGally` and the `lmodel2` libraries.  Make sure that they are both installed:
```{r, eval=FALSE}
install.packages("GGally")
install.packages("lmodel2")
```

```{r, echo=FALSE}
library(dplyr)
library(tidyr)
```

Now load required libraries:
```{r}
library(GGally)
library(ggplot2)
library(lmodel2)
```

# Basic linear fitting with and without weights 

## Reminder of the lecture
We have data (x, y), where x is a deterministic variable and y is a random variable that is dependent on x. 
We fit a linear regression line to the data with slope $\beta_1$ and intercept $\beta_0$

by finding the minimum of the RSS

$RSS(\beta_0, \beta_1) = \sum_{i=1}^{n} (y_{i} - \beta_0 - \beta_1 x_i)^2$

with is found at

* $\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i-\bar{x})^2}$
* $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$

This equation requires that the variance in y is the same for all $x_i$

If this is not the case, then we apply weighted regression, by minimizing:

$RSS(\beta_0, \beta_1) = \sum_{i=1}^{n} w_i (y_{i} - \beta_0 - \beta_1 x_i)^2$

with $w_i = 1 / \sigma_i$

## Syntax

In R, the slope and intercept can be calculated by the command:

```{r, eval=FALSE}
b1 = cov(x,y)/var(x)
b0 = mean(y) – b1*mean(x)
```

For the general linear regression, use `lm()`

syntax (explain, e.g., what is meant by formula etc…)

**TODO** we just need `formula`, `weights`, and `data`, right?

```{r, eval=FALSE}
z = lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
```

* `formula` - a symbolic description of the model to be fitted
    * A typical model has the form `response ~ terms` where response is the (numeric) response vector and terms is a series of terms which specifies a linear predictor for response.
    * A terms specification of the form `first + second` indicates all the terms in first together with all the terms in second with duplicates removed.
* `data` - the data frame used as input.  The names you specify in `formula` will correspond to the column names in this data frame.
   
## Example
Make a nice example:
e.g., data x, y, with y error bars s 

### Anscombe's Quartet]

For this practical, we'll use a data frame containing [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) as an example.  The data consists of four hypothetical collections of data that each have x and y values.  For example, the first data set uses the columns `x1` and `y1`, the second uses `x2` and `y2`, etc.  You can type `?anscombe` into the console to find out more about the data set.

We'll first look at some basic statistical properties.  In the examples below, we use `c()` to group together the results into a vector so that they're easier to see.  

As you can see, the mean of x values are the same:
```{r}
c(mean(anscombe$x1), mean(anscombe$x2), mean(anscombe$x3), mean(anscombe$x4))
```

as is the variance of x values:
```{r}
c(var(anscombe$x1), var(anscombe$x2), var(anscombe$x3), var(anscombe$x4))
```

The mean of y values is the same to several decimal places:
```{r}
c(mean(anscombe$y1), mean(anscombe$y2), mean(anscombe$y3), mean(anscombe$y4))
```

The variance of y values is also very similar:
```{r}
c(var(anscombe$y1), var(anscombe$y2), var(anscombe$y3), var(anscombe$y4))
```

However, when we plot the different data sets, we see that they all look quite different:
```{r, echo=FALSE}
anscombe_2cols = data.frame(x = c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4),
                            y = c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4),
                            series = c(rep("Data 1", 11), rep("Data 2", 11), rep("Data 3", 11), rep("Data 4", 11)))

ggplot(anscombe_2cols, aes(x=x,y=y)) + geom_point() + facet_wrap(~series)
```

For the example below, we'll use the x and y values from the first data set in Anscombe's Quartet.

```{r}
anscombe1 = data.frame(x = anscombe$x1, 
                       y = anscombe$y1)
```

### Perform a linear regression
Now we perform a linear regression on the data:
```{r}
z = lm(y~x, data=anscombe1)
```

* `data=anscombe1` means that we want to use the `anscombe1` data frame that we just created
* `y~x` says that we're trying to predict a response (the value `y`) given the value of a term (`x`).  Here `x` and `y` correspond to the actual column names of the data frame.  If we were to us a different data frame (like `mtcars`) we would update the formula (for example to predict miles per gallon given horse power, we would use `mpg~hp`).

### Basic regression output 
Using `summary` we can get an overview of the results of the regression
```{r}
summary(z)
```

### Error bar plots in ggplot

**TODO** check that this explanation isn't confusing.  We just need something to create error bars

For some data sets, you will also have information available on the error for particular values.  Anscombe's quartet doesn't contain this information, but for the sake of demonstrating the programming syntax to do this, we add a `y_error` column using random values.  

```{r}
anscombe1$y_error = runif(nrow(anscombe1))
```

Now we perform a scatter plot using `geom_point`, but now we also add `geom_errorbar` to add error bars on top of those points.  The code `ymin = y - y_error` and `ymax = y + y_error` shows how we use the values from the `y` and `y_error` columns to calculate the top and bottom of the error bars.  

```{r}
ggplot(anscombe1, aes(x=x, y=y)) + geom_point() + 
 geom_errorbar(aes(ymin = y - y_error,
                   ymax = y + y_error,
                   width=0.3)) # width changes the width of the horizontal bar at the top of the error bar
```

In this example, the error bars are "mirrored" around the value of `y`.  Since we specify the value of `ymin` independently of `ymax`, then it's possible that you could use a different formula and/or column so that the observed value of `y` does not lie directly in the middle of the error bars but is skewed to one side.

### Add a regression line to the error bar plot 

We will reuse the same code from above, but will now add in a regression line.  To do this, we need to use the coefficients that were found from the linear regression.
```{r}
z$coefficients
```

As you can see, the first value is the intercept.  The second value is labelled `x` although this refers to the slope of the regression line.  We will use these values directly in the `geom_abline` command:

```{r}
ggplot(anscombe1, aes(x=x, y=y)) + geom_point() + 
  geom_errorbar(aes(ymin=y - y_error,
                    ymax=y + y_error,
                    width=0.3)) + 
  geom_abline(intercept = z$coefficients[1], slope = z$coefficients[2])
```

Alternatively, we can also use `geom_smooth` to perform a linear regression, although for the examples in this practical, we'll use `geom_abline`.

```{r}
ggplot(anscombe1, aes(x=x, y=y)) + geom_point() + 
  geom_errorbar(aes(ymin=y - y_error,
                    ymax=y + y_error,
                    width=0.3)) + 
  geom_smooth(aes(x=x, y=y), method="lm", formula=y~x, se=FALSE)
```

One thing which we'll come back to later is that the linear regressions for the four data sets in Anscombe's Quartet are nearly identifical:
```{r, echo=FALSE}
ggplot(anscombe_2cols, aes(x=x,y=y)) + 
  geom_point() + 
  geom_smooth(aes(x=x, y=y), 
               method="lm", formula=y~x, 
              se=FALSE, fullrange=TRUE) + 
  facet_wrap(~series)
```

## Exercise

For this exercise, download the data [Practical6_Data.txt](https://raw.githubusercontent.com/cbdavis/DASM/master/data/Practical6_Data.txt) and load it into R.

read in data 1, make an error bar plot, do a fit with and without weights, add the regression line to the plot and look at the regression output 


### Make an error bar plot

You should see:

```{r, echo=FALSE}
data1 <- read.csv("./data/Practical6_Data.txt")

ggplot(data1, aes(x=x_values, y_values)) + geom_point() + 
 geom_errorbar(aes(ymin=y_values - y_errors,
                   ymax=y_values + y_errors,
                   width=0.005)) # width changes the width of the horizontal bar at the top of the error bar
```

### Perform a linear regression
Using `summary` on the results, you should see:

```{r, echo=FALSE}
#fit a "normal" fit
model5 <-lm(y_values~x_values, data1)
#abline(model5) 

#to get the information about the fit
summary(model5)
```

### Add the regression line to the error bar plot
```{r, echo=FALSE}
ggplot(data1, aes(x=x_values, y_values)) + geom_point() + 
  geom_errorbar(aes(ymin=y_values - y_errors,
                    ymax=y_values + y_errors,
                    width=0.005)) + 
  geom_abline(intercept = model5$coefficients[1], 
              slope = model5$coefficients[2])
```

### Perform a linear regression with weights specified

**TODO** how much to tell the students here?

For the linear regression, we will specify the weights as `1/(data1$y_errors^2)`

The summary of the linear regression results should show:
```{r, echo=FALSE}
# take into account the error bars
model6 <-lm(y_values~x_values, data = data1, 
            weights = 1/(data1$y_errors^2))

# to get information about the fit
summary(model6)
```


### Plot both linear regressions (with and without weights)
Remember that with ggplot, you can append commands like `+ geom_abline(..., color="blue") + geom_abline(..., color="red")`

```{r, echo=FALSE}
ggplot(data1, aes(x=x_values, y_values)) + geom_point() + 
 geom_errorbar(aes(ymin=y_values - y_errors,
                   ymax=y_values + y_errors,
                   width=0.005)) + # width changes the width of the horizontal bar at the top of the error bar
 geom_abline(aes(slope = model5$coefficients[2], intercept=model5$coefficients[1]), color="red") +
 geom_abline(aes(slope = model6$coefficients[2], intercept=model6$coefficients[1]), color="blue")
```

# Regression diagnostic

## Residuals (ei)

### Reminder from the lecture

* $E(e_i|X_i) = 0$ (residuals should have mean 0 for all $X_i$ )
* $e_i$ is normally distributed (residuals are normally distributed with mean = 0 and variance $Var(e_i) = s_2$, for all $e_i$

### Syntax

Plot residuals against fitted values

Again we'll use the anscombe1 data frame that we created above, using the same code for the linear regression:

```{r}
z = lm(y~x, anscombe1)
```
then 

get the fitted values
```{r}
fit <- fitted(z)
print(fit)
```

get the residuals
```{r}
res <- residuals(z)
print(res)
```

Add in columns to the anscombe1 data frame for the fit and res values
```{r}
anscombe1$fit = fit
anscombe1$res = res
```

We can see that the fitted values follow the regression line:
```{r}
ggplot(anscombe1) + 
  geom_point(aes(x=x, y=y, color="original value")) + 
  geom_point(aes(x=x, y=fit, color="fitted value")) + 
  geom_abline(intercept = z$coefficients[1], slope=z$coefficients[2])
```

```{r}
ggplot(anscombe1) + 
  geom_point(aes(x=x, y=res, color="original value"))
```

Putting this all together we can see how subtracting the residual from the original `y` values gives us the fitted values in `fit`

`geom_segment` allows us to draw line segments from the points defined at `x`,`y` to the point at `xend`,`yend`

```{r}
ggplot(anscombe1) + 
  geom_point(aes(x=x, y=y, color="original value"), size=3) + 
  geom_point(aes(x=x, y=fit, color="fitted value"), size=3) + 
  geom_segment(aes(x=x, xend=x, y=y, yend=y-res)) + 
  geom_abline(intercept = z$coefficients[1], slope=z$coefficients[2])
```

Plot residual against fitted values
```{r}
ggplot(anscombe1, aes(x=fit, y=res)) + geom_point() + ggtitle("residual vs. fitted values")
```

Perform a qqnorm plot of residuals to see if they are normally distributed
```{r}
qqnorm(res)
qqline(res)
```

### Exercise

Make the plots of fitted values vs residuals for the data from Practical6_Data.txt 

**TODO** should we do this for both weighted and unweighted regressions?

Linear regression without weights

```{r}
data1 <- read.csv("./data/Practical6_Data.txt")

model5 <-lm(y_values~x_values, data1)

#get the fitted values
fitted5 <- fitted(model5)
#get the residuals
res5 <- residuals(model5)

data1$res5 = res5
data1$fitted5 = fitted5

# plot residual against fitted values
plot(fitted5, res5)

ggplot(data1, aes(x=fitted5, y=res5)) + geom_point()
```

Make a qnorm plot of the residuals

```{r}
qqnorm(res5)
qqline(res5)
```


Linear regression with weights (performed same as earlier in the practical)
```{r}
model6 <-lm(y_values~x_values, data = data1, 
            weights = 1/(data1$y_errors^2))

#get the fitted values
fitted6 <- fitted(model6)
#get the residuals
res6 <- residuals(model6)

data1$res6 = res6
data1$fitted6 = fitted6

# plot residual against fitted values
ggplot(data1, aes(x=fitted6, y=res6)) + geom_point()
```

Make a qnorm plot of the residuals
```{r}
qqnorm(res6)
qqline(res6)
```


## Cook's distance

### Reminder of the lecture

The cook’s distance of point $X_i$ is a measure of the difference in regression parameters when the point is included or omitted. If omitting a point changes the regression parameters strongly, this point has a large Cook’s distance.

To see quantitatively if the Cook's distance of a data point is too large and it has an undue influence on the fit you need to compare it with a theoretical cut-off

This cutoff value can be calculated with the F-statistic as:
fF(0.5, p, n-p) with p … fitted parameters, n … number of data points

### Syntax

Here we use the anscombe1 data frame again:

**TODO** show the different values for anscombe's quartet here

```{r}
#Cook's distance

z = lm(y~x, data=anscombe1)
cook <- cooks.distance(z)
plot(fitted(z), cook, ylim = c(0,1))

#cut-off value

cut <- qf(0.5, 2, length(data1$x_values)-2)
abline(cut,0)
```

**TODO** one of the points here for cook's distance is NaN - what is the cause?  This is for the 4th data set in Anscombe's quartet

```{r, echo=FALSE}
cook1 = cooks.distance(lm(y~x, anscombe_2cols %>% filter(series == "Data 1")))
cook2 = cooks.distance(lm(y~x, anscombe_2cols %>% filter(series == "Data 2")))
cook3 = cooks.distance(lm(y~x, anscombe_2cols %>% filter(series == "Data 3")))
cook4 = cooks.distance(lm(y~x, anscombe_2cols %>% filter(series == "Data 4")))

cut <- qf(0.5, 2, 11-2)

anscombe_2cols$cooks_distance = c(cook1, cook2, cook3, cook4)

ggplot(anscombe_2cols, aes(x=x, y=cooks_distance)) + 
  geom_point() + 
  facet_wrap(~series) + geom_hline(yintercept = cut)
```

### Exercise

For the un-weighted and weighted fit, compare the cooks distance 

```{r}
#Cooks distance
cook5 <- cooks.distance(model5)
plot(fitted(model5), cook5, ylim = c(0,1))
cut5 <- qf(0.5, 2, length(data1$x_values)-2)
abline(cut5,0)
```

```{r}
#Cooks distance
cook6 <- cooks.distance(model6)
plot(data1$x_value, cook6, ylim= c(0,1))
cut6 <- qf(0.5, 2, length(data1$x_values)-2, lower.tail = FALSE)
abline(cut6,0)
```

## Automatic diagnostic plots

```{r, eval=FALSE}
# you can also automatically make diagnostic plots by:
plot(z)
```


### Try it yourself for the two fits of data 1

**TODO** notes - last plot is a diff way of looking at Cook's distance
We don't cover the Scale location

When to use plot, when to use ggplot

Don't pay much attention to last plot - need to check the interpretation.  With the weighted lm, the cooks' distance plot shows points with higher cook's distance than observed above.  

might matter whether calculate weighted versus standard residuals.

```{r}
plot(model5)
plot(model6)
```


# Reduced major axis regression

## Reminder of the lecture 

Reduced major axis regression minimizes the area of the triangles formed by the observations and the line:

$\beta_1 = s_x / s_y$

$\beta_0$ is calculated like in the normal regression

## Syntax

**TODO** for this example, use the data1 data set from above for model8

```{r, eval=FALSE}
model <-lmodel2(y~x, data = dataframe, range.y = "interval", 
                 range.x = "interval", nperm = 100)
```

Arguments:

* `formula`	- A formula specifying the bivariate model, as in lm and aov.
* `data` - A data frame containing the two variables specified in the formula.
* `range.y`, `range.x` - Parametres for ranged major axis regression (RMA). If range.y = NULL and range.x = NULL, RMA will not be computed. If only one of them is NULL, the program will stop. If range.y = "relative": variable y has a true zero (relative-scale variable). If range.y = "interval": variable y possibly includes negative values (interval-scale variable). If range.x = "relative": variable x has a true zero (relative-scale variable). If range.x = "interval": variable x possibly includes negative values (interval-scale variable)
* `nperm` - Number of permutations for the tests. If nperm = 0, tests will not be computed.



```{r}
model <-lmodel2(y_values~x_values, data = data1, range.y = "interval", 
                 range.x = "interval", nperm = 100)

#Choose the right regression coefficients from the output matrix
#model$regression.results (Remember RMA is the fourth model to fitted)

model$regression.results
```

**TODO** dplyr method of retrieving intercept & slope vs. index based 

```{r}
intercept = model$regression.results %>% filter(Method == "RMA") %>% select(Intercept) %>% unlist()
slope = model$regression.results %>% filter(Method == "RMA") %>% select(Slope) %>% unlist()

a <- model$regression.results[4,2]
b <- model$regression.results[4,3]

# make a summary of the regression model
summary(model)

# abline(a,b, col = "pink")
ggplot(data1, aes(x=x_values, y=y_values)) + geom_point() + geom_abline(slope=slope, intercept=intercept)
```


## Try it yourself: make a Reduced major axis regression for x and y in data 1 and add the line to the plot 

```{r, eval=FALSE}
a <- model8$regression.results[4,2]
b <- model8$regression.results[4,3]
abline(a,b, col = "pink")
summary(model8)
```

# Multiple regression

## Reminder of the lecture 9

Yi =  b0 + b1*X1i + b2*X2i +  b3*X3i + … +  ei 

Interpretation of the parameters:

* If bj  > 0, then j stands for the average increase of the response when predictor xj increases with one unit, holding all other variables constant.
* If bj  < 0, then j stands for the average decrease of the response when predictor xj increases with one unit, holding all other variables constant.

An anova test can be done to see if the removing one variable significantly changes the fit. If p-value of the anova test <0.05, you should keep the variable (as a rule of thumb)
 
## Syntax 

### Linear model as a function of several parameters
 say you have several variables (y, x1, x2, x3, …) in the data frame `dat`

```{r, eval=FALSE}
z = lm(y~ x1 + x2 + x3 + …, data = dat) 
```

### A pairs plot

if your variables are in a data frame called dat 

```{r, eval=FALSE}
ggpairs(dat)
```

### Anova test

if 
z1 = lm(y~ x1 + x2 + x3, data = dat) 
and 
z2 = lm(y~ x1 + x2, data = dat) 

anova(z1,z2)


## Exercise

**TODO** make sure the students know step by step what do do

To do chris: make a step by step description of what they should do … 

This is what they should get out: 

the example from the lecture 8

**TODO** some of the plots below are easier with the `plot` function since they override the standard `plot` method with a custom method that is based on the type of data being analyzed.  I don't think this occurs with ggplot

### Get R data set stackloss
i.e get the pacakge datasets
Brownlee, K. A. (1960, 2nd ed. 1965) Statistical Theory and
Methodology in Science and Engineering. New York: Wiley. pp. 491???500.

###look at a summary of the data set
```{r, echo=FALSE}
summary(stackloss)
```

### make a pairs plot
**TODO** make sure to cover ggpairs somewhere above
```{r, echo=FALSE}
ggpairs(stackloss)
```

###Fit a linear model using all variables
```{r, echo=FALSE}
mul_fit1 <- lm(stack.loss ~ Acid.Conc. + Air.Flow + Water.Temp, data = stackloss)
```

###look at the ouput
```{r, echo=FALSE}
summary(mul_fit1)
```

###plot the standard diagnostic plots
```{r, echo=FALSE}
plot(mul_fit1)
```

### make our normal Cook's distance plot by hand and add the cutoff line
**TODO** convert below to ggplot
```{r, echo=FALSE}
plot(fitted(mul_fit1), cooks.distance(mul_fit1), ylim = c(0,1))
cut <- qf(0.5,4,17)
abline(a= cut, b = 0)
``` 

###Leave out the least important parameter Acid concentration
```{r, echo=FALSE}
less_fit<- lm(stack.loss~Air.Flow+Water.Temp, data = stackloss)
summary(less_fit)
```

### see if that made a difference for the overall model
```{r, echo=FALSE}
anova(mul_fit1,less_fit)
```

###Now leave out water temp
```{r, echo=FALSE}
least_fit<- lm(stack.loss~Air.Flow, data = stackloss)
summary(least_fit)
anova(less_fit, least_fit)
```

###fits of individual varaibles

**TODO** put in ggplot examples here

```{r, echo=FALSE}
fit_air <- lm(stack.loss~Air.Flow, data = stackloss)
sum_air <- summary(fit_air)
R2_air <-sum_air$r.squared
plot(stack.loss~Air.Flow, data = stackloss)
abline(fit_air)

fit_temp <- lm(stack.loss~Water.Temp, data = stackloss)
sum_temp <- summary(fit_temp)
R2_temp <-sum_temp$r.squared
plot(stack.loss~Water.Temp, data = stackloss)
abline(fit_temp)

fit_ind <- lm(Air.Flow~Water.Temp, data = stackloss)
sum_ind <- summary(fit_ind)
R2_ind <-sum_ind$r.squared
plot(Air.Flow~Water.Temp, data = stackloss)
abline(fit_ind)
```


Could leave out different variables and see if it makes a difference