---
title: "Practical 6"
author: "Chris Davis"
date: "October 10, 2016"
output:
  html_document:
    number_sections: yes
    toc: yes
---

# Basic linear fitting with and without weights 

## Reminder of the lecture
We have data (x, y), where x is a deterministic variable and y is a random variable that is dependent on x. 
We fit a linear regression line to the data with slope and intercept 

by finding the minimum of the RSS


with is found at


This equation requires that the variance in y is the same for all xi 

If this is not the case, then we apply weighted regression, by minimizing:



with wi = 1/i 


## R syntax:

a) For the In R the slope and intercept can be calculated by the command:
In R:
```{r, eval=FALSE}
b1 = cov(x,y)/var(x)
b0 = mean(y) – b1*mean(x)
```


b) For the general linear regression, use lm()
syntax (explain, e.g., what is meant by formula tec…)

```{r}
xx = lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
```

not used in this course

Make a nice example:
e.g., data x, y, with y error bars s 

```{r}
z = lm(x~y, weights = 1/s)
```

c) look at the basic regression output 
> summary(z)

d) explain how to plot an error bar plot in gg-polt 

e) add the regression line to the error bar plot 
(note to chris with normal plotting you could just do abline(reg), I’m not totally sure, if this also works in ggplot)

#abline plots the regression line on the plot 
#usage = abline(a,b), with a = intercept, b = slope


## Do it yourself:

read in data 1, make an error bar plot, do a fit with and without weights, add the regression line to the pot  and look at the regression output 

```{r}
data1 <- read.csv("file path/data1.txt")

#plot an error bar graph

#fit a "normal" fit
model5 <-lm(y_values~x_values, data1)
abline(model5) 


#to get the information about the fit
summary(model5)

# take into account the error bars
model6 <-lm(y_values~x_values, data= data1, 
            weights = 1/data1$y_errors)

# to get information about the fit
summary(model6)

#plot an error bar graph
# in package Hmisc
errbar(data1$x_values, data1$y_values, data1$y_values+data1$y_errors, 
       data1$y_values-data1$y_errors, cap = 0.015, ylab = "y", xlab = "x")
# add the lines from both fits
abline(model5, col = "green") 
abline(model6, col = "blue")
# add a legend
legend(0.05, 0.45, c("data", "standard LS fit", "weighted LS fit"), 
       col = c("black", "green", "blue"), lty= c(NA,1,1), pch = c(16, NA, NA))
```

# Regression diagnostic

## Residuals (ei)

### Reminder from the lecture


1. E(ei|Xi) = 0 (residuals should have mean 0 for all Xi )
2. ei  is normally distributed (residuals are normally distributed with mean = 0 and variance Var(ei ) = s2, for all ei  


### In R (explanation of syntax)

Plot residuals against fitted values

say you have values x and y, and you want to make a linear fit as a function of y 
> z = lm(y~x)
then 

#get the fitted values
fit <- fitted(z)
#get the residuals
res <- residuals(z)
# plot residual against fitted values
plot(fit, res)

#qqnorm plot of residuals 
qqnorm(res)



### Do it yourself

Make the plots of fitted values vs residuals for data1
and a qnorm plot of the residuals

```{r}
#get the fitted values
fitted5 <- fitted(model5)
#get the residuals
res5 <- residuals(model5)

# plot residual against fitted values
plot(fitted5, res5)


#get the fitted values
fitted6 <- fitted(model6)
#get the residuals
res6 <- residuals(model6)

# plot residual against fitted values
plot(fitted6, res6)
```


## Cooks distance

### reminder of the lecture

The cook’s distance  of point Xi is a measure of the difference in regression parameters when the point is included or omitted. If omitting a point changes the regression parameters strongly, this point has a large Cook’s distance.

To see quantitatively if the Cooks distance of a data point is too large and it has an undue influence on the fit you need to compare it with a theoretical cut-off
This cutoff value can be calculated with the F-statistic as:
fF(0.5, p, n-p) with p … fitted parameters, n … number of data points

### Syntax:

```{r}
#Cooks distance

z = lm(y~x)
cook <- cooks.distance(z)
plot(fitted(model5), cook5, ylim = c(0,1))

#cut-off value

cut <- qf(0.5, 2, length(data1$x_values)-2)
abline(cut5,0)
```


### Do it yourself: For the un-weighted and weighted fit, compare the cooks distance 

```{r}
#Cooks distance
cook5 <- cooks.distance(model5)
plot(fitted(model5), cook5, ylim = c(0,1))
cut5 <- qf(0.5, 2, length(data1$x_values)-2)
abline(cut5,0)
```

```{r}
#Cooks distance
cook6 <- cooks.distance(model6)
plot(data1$x_value, cook6, ylim= c(0,1))
cut6 <- qf(0.5, 2, length(data1$x_values)-2, lower.tail = FALSE)
abline(cut6,0)
```

## automatic diagnostic plots

```{r}
# you can also automatically make diagnostic plots by:
plot(z)
```


### Try it yourself for the two fits of data 1
```{r}
plot(model5)
plot(model6)
```


# reduced major axis regression

## Reminder of the lecture 

Reduced major axis regression minimizes the area of the triangles formed by the observations and the line:

	 = sx / sy   

  is calculated like in the normal regression


## Syntax:

```{r}
# for this you need to install and select the 
# package lmodel2

x <-lmodel2(y~x, data = …, range.y = "interval", 
                 range.x = "interval", nperm = 100)

#Choose the right regression coefficients from the output matrix
#model8$regression.results (Remember RMA is the fourth model to fitted)

a <- model8$regression.results[4,2]
b <- model8$regression.results[4,3]

# make a summary of the regression model
summary(model8)
```


## Try it yourself: make a Reduced major axis regression for x and y in data 1 and add the line to the plot 

a <- model8$regression.results[4,2]
b <- model8$regression.results[4,3]
abline(a,b, col = "pink")
summary(model8)

# multiple regression

## reminder of the lecture 9


Yi =  b0 + b1*X1i + b2*X2i +  b3*X3i + … +  ei 

Interpretation of the parameters:
If bj  > 0, then j stands for the average increase of the response when predictor xj increases with one unit, holding all other variables constant.
If bj  < 0, then j stands for the average decrease of the response when predictor xj increases with one unit, holding all other variables constant.

An anova test can be done to see if the removing one variable significantly changes the fit. If p-value of the anova test <0.05, you should keep the variable (as a rule of thumb)
 
## Syntax 

### linear model as a function of several parameters
 say you have several variables (y, x1, x2, x3, …) in the data frame dat 

z = lm(y~ x1 + x2 + x3 + …, data = dat) 

4.2.2 a pairs plot

if your variables are in a data frame called dat 
> pairs(dat) 

### Anova test

if 
z1 = lm(y~ x1 + x2 + x3, data = dat) 
and 
z2 = lm(y~ x1 + x2, data = dat) 

anova(z1,z2)


## Do it yourself 


To do chris: make a step by step description of what they should do … 

This is what they should get out: 

the example from the lecture 8

```{r}
Get R data set stackloss 
#i.e get the pacakge datasets
#Brownlee, K. A. (1960, 2nd ed. 1965) Statistical Theory and 
#Methodology in Science and Engineering. New York: Wiley. pp. 491???500.

#look at a summary of the data set
summary(stackloss)
#plot all the variables against each other
pairs(stackloss)

#Fit a linear model using all variables
mul_fit1 <- lm(stack.loss ~ Acid.Conc. + Air.Flow + Water.Temp
               , data = stackloss)
#look at the ouput
summary(mul_fit1)

#plot the standard diagnostic plots
plot(mul_fit1)

# make our normal Cook's distance plot
plot(fitted(mul_fit1), cooks.distance(mul_fit1), ylim = c(0,1))
#calculate the cut-off: 4 parameters fitted

cut <- qf(0.5,4,17)
abline(a= cut, b = 0)

#Leave out the least important parameter Acid concentration
less_fit<- lm(stack.loss~Air.Flow+Water.Temp, data = stackloss)
summary(less_fit)
# see if that made a difference for the overall model
anova(mul_fit1,less_fit)

#Now leave out water temp
least_fit<- lm(stack.loss~Air.Flow, data = stackloss)
summary(least_fit)
anova(less_fit, least_fit)


#fits of individual varaibles
fit_air <- lm(stack.loss~Air.Flow, data = stackloss)
sum_air <- summary(fit_air)
R2_air <-sum_air$r.squared
plot(stack.loss~Air.Flow, data = stackloss)
abline(fit_air)


fit_temp <- lm(stack.loss~Water.Temp, data = stackloss)
sum_temp <- summary(fit_temp)
R2_temp <-sum_temp$r.squared
plot(stack.loss~Water.Temp, data = stackloss)
abline(fit_temp)

fit_ind <- lm(Air.Flow~Water.Temp, data = stackloss)
sum_ind <- summary(fit_ind)
R2_ind <-sum_ind$r.squared
plot(Air.Flow~Water.Temp, data = stackloss)
abline(fit_ind)

```