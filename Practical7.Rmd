---
title: "Practical 7"
author: "Chris Davis"
date: "October 17, 2016"
output:
  html_document:
    number_sections: yes
    toc: yes
---

**TODO** put all the contents of the practical ppt in the doc here for stl decomposition only.  Can leave out the other stuff

**TODO** need consistent format - in some cases use dataframe with time series column.  In others, just use a vector.  This can cause some confusion with the students as some of the functions won't raise an error, but will evaluation all columns in the data frame.

# Load and Install Required Libraries

We need to install the `forecast` package for the `ma` (moving average) function which we'll use later in this practical.
```{r, eval=FALSE}
install.packages("forecast")
```

Load the requried libraries for this practical:
```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(forecast)
```


# Creating Time Series Objects

**TODO** explain the `$x` part

As we showed in the previous practical, we can create a sequence of dates (using `seq`), starting at one date with a defined time interval for each step.

**TODO** use the Pr_20May1.csv data set

```{r}
Tvalues = read.csv("./data/Pr_20May1.csv")
#Tvalues = Tvalues$x 

Tvalues$t = seq(from = as.Date("1946-01-01"), to = as.Date("2014-12-01"), by = "1 month")

# TODO remove this?
t_seq = seq(from = as.Date("1946-01-01"), to = as.Date("2014-12-01"), by = "1 month")

plot(t_seq, Tvalues$x, type = "l")
```

## Exercise

Download the file [Practical7_time_series.txt](https://raw.githubusercontent.com/cbdavis/DASM/master/data/Practical7_time_series.txt) and load it into R.

This contains daily data on the internet traffic from 20. November 2004 to 27. January 2005

Hint: Reading in the text file produces a data frame. For the following analyses it is better to use the data as a simple vector: series <- data$x

Make a vector t of the times corresponding to each data point and plot the time series vs. t. Can you see a seasonality in the data? which period is it?

```{r, echo=FALSE}
series2 = read.csv("./data/Practical7_time_series.txt")
series2$t = seq(from = as.Date("2004-11-19"), to = as.Date("2005-01-26"), by = "1 day")

ggplot(series2, aes(x=t, y=x)) + geom_line()
```

**TODO** take out text below?
There is a weekly seasonality

## Using `ts()`
We can also use the `ts` function to create time series objects.  The relevant syntax is shown below and you can learn more by typing `?ts` into the console.

```{r, eval=FALSE}
ts(data = NA, start = 1, end = numeric(), frequency = 1,
   deltat = 1)
```

Arguments:

* `data` - a vector or matrix of the observed time-series values. 
* `start` - the time of the first observation. Either a single number or a vector of two integers, which specify a natural time unit and a (1-based) number of samples into the time unit. See the examples for the use of the second form.
* `end` - the time of the last observation, specified in the same way as start.
* `frequency` - the number of observations per unit of time.
* `deltat` - the fraction of the sampling period between successive observations; e.g., 1/12 for monthly data. Only one of frequency or deltat should be provided.

To perform the same operation as above, we can also:
```{r}
t_ser <- ts(Tvalues$x, start = c(1946, 1), freq = 12)
plot(t_ser)
```

Can also define without a start date.  Using `freq=12` means that you have 12 observations per cycle, so the period of the seasonal variation is 12.  Now the values on the x axis correspond to the cycle number, which in this case refers to the number of years after the first observation. 

```{r}
t_ser1 <- ts(Tvalues$x, freq = 12)
plot(t_ser1)
```

# Moving-average smoothing

We will now use the `ma` function for moving-average smoothing.

Make sure to load the `forecast` library:

```{r}
library(forecast)
```

The syntax is:

```{r, eval=FALSE}
ma(x, order, centre=TRUE)
```

Arguments:

* `x` - Univariate time series
* `order` - Order of moving average smoother
* `centre` - If TRUE, then the moving average is centred for even orders.

initial example

```{r}
ma5 = ma(Tvalues$x, order = 5)
ma13 = ma(Tvalues$x, order = 13)
```


let's put everything in a data frame
```{r}
Tvalues_with_ma = Tvalues %>% 
  mutate(ma5 = ma(x, order = 5), 
         ma13 = ma(x, order = 13))

#plot(t_seq[1:60], Tvalues[1:60], type = "l")
#lines(t_seq[1:60], ma5[1:60], col = "red")
#lines(t_seq[1:60], ma13[1:60], col = "green")

ggplot(Tvalues_with_ma, aes(x=t)) + 
  geom_line(aes(y=x, color="original data"), size=2) + 
  geom_line(aes(y=ma5, color="ma5")) + 
  geom_line(aes(y=ma13, color="ma13")) + 
  xlim(c(as.Date("1980-01-01"), 
         as.Date("1984-01-01"))) # Just show data from 1980-1984, otherwise the plot is too cluttered

```

**TODO** change explanation to refer to the example above (currently refers to example below)

What you 
m = 3  … red; 3 data points are averaged (1 to the left and right of the data point)
m = 7  … red; 7 data points are averaged (3 to the left and right of the data point)
(this smoothes out the weekly cycle)
m = 15  … red; 15 data points are averaged (7 to the left and right of the data point)



## Exercise

Calculate a moving average with order m = 3, 7, and 15 (see lecture) and add it to the plot. Explain the order m means. 

Use series2 = read.csv("./data/Practical7_time_series.txt")

```{r, echo=FALSE}
series2_with_ma = series2 %>% 
  mutate(ma3 = ma(x, order = 3), 
         ma7 = ma(x, order = 7), 
         ma15 = ma(x, order = 15))

ggplot(series2_with_ma, aes(x=t)) + 
  geom_line(aes(y=x, color="original data"), size=2) + 
  geom_line(aes(y=ma3, color="ma3")) +
  geom_line(aes(y=ma7, color="ma7")) + 
  geom_line(aes(y=ma15, color="ma15"))
  
```


# Lagging time values

**TODO** - do this with dplyr?  should be possible
see dplyr::lag

An easy way to do this is with the `lag` command and `dplyr`

Here we shift the values for `x` in `Tvalues` by three months.  We use the `mutate` function to create a new column named `lag_x` in the data frame.  The `lag` function simply takes as imput a vector and shifts its values by the number specified.  Take for example a vector containing the numbers 1, 2, ..., 10:

```{r}
a = c(1:10)
print(a)
lag(a, 3)
```

As you can see we push all the values over three places and fill in the left side with `NA`.  The values 8, 9 and 10 are discarded since they are pushed out.

Now we create a new data frame with the results of the operation:
```{r}
Tvalues_lag3 = Tvalues %>% mutate(lag_x = lag(x, 3))
```

Looking at the first few rows of the data frame, we can see how `NA` values are inserted at first, with the lagged values starting at row 4:
```{r}
head(Tvalues_lag3)
```

Below we plot the results for several different lag amount.  Instead of creating a new data frame each time, we just pass the results directly to ggplot using `data = Tvalues %>% mutate(lag_x = lag(x, 3))`

```{r}
ggplot(data = Tvalues %>% mutate(lag_x = lag(x, 3)), 
       aes(x=x, y=lag_x)) + 
  geom_point()

# data shifted 6 months
ggplot(data = Tvalues %>% mutate(lag_x = lag(x, 6)), 
       aes(x=x, y=lag_x)) + 
  geom_point()

# data shifted 10 months
ggplot(data = Tvalues %>% mutate(lag_x = lag(x, 10)), 
       aes(x=x, y=lag_x)) + 
  geom_point()

# data shifted 12 months
ggplot(data = Tvalues %>% mutate(lag_x = lag(x, 12)), 
       aes(x=x, y=lag_x)) + 
  geom_point()
```
 
## Exercise

Show the actual correlation (scatter) plot that is used to calculate the ACF at lag 10. 

```{r, echo=FALSE}
ggplot(data = series2 %>% mutate(lag_x = lag(x, 10)), 
       aes(x = x, y = lag_x)) + 
  geom_point()
```


# Auto- and Cross- Covariance and -Correlation Function Estimation

## Reminder of the lecture

**TODO** explain acf syntax

```{r, eval=FALSE}
acf(x, lag.max = NULL,
    type = c("correlation", "covariance", "partial"),
    plot = TRUE, na.action = na.fail, demean = TRUE, ...)
```

Arguments:

* `x, y` - a univariate or multivariate (not ccf) numeric time series object or a numeric vector or matrix, or an "acf" object.
* `lag.max` - maximum lag at which to calculate the acf. Default is `10*log10(N/m)` where `N` is the number of observations and m the number of series. Will be automatically limited to one less than the number of observations in the series.
* `type` - character string giving the type of acf to be computed. Allowed values are `"correlation"` (the default), `"covariance"` or `"partial"`. Will be partially matched.

For this practical, we will only use `type="correlation"`.


Use the Pr_20May1.csv

**TODO** make sure that reload the data each time, or something easier
```{r}
Tvalues = read.csv("./data/Pr_20May1.csv")
acf_T = acf(Tvalues,type = "correlation", lag.max = 64)
acf_10 = acf_T$acf[10]
```

## Exercise

Calculate the autocorrelation function (ACF) of the time series and plot it up to lag 30. 

What variability is mainly reflected in the ACF?

use this Practical7_time_series.txt

```{r, echo=FALSE}
series2 = read.csv("./data/Practical7_time_series.txt")
acf.ser2<- acf(series2, type = "correlation", lag.max = 30)
```

The main signal you see in the ACF is the large-scale variation

# Decomposition

## Reminder from the lecture

Syntax

```{r, eval=FALSE}
decompose(x, type = c("additive", "multiplicative"), filter = NULL)
```

* `x` - A time series.
* `type` - The type of seasonal component. Can be abbreviated.
* `filter` - A vector of filter coefficients in reverse time order (as for AR or MA coefficients), used for filtering out the seasonal component. If NULL, a moving average with symmetric window is performed.

By default, an additive model is performed if no value is specified for `type`.

**TODO** make sure it's clear where the data comes from

## Classical
```{r}
output = decompose(t_ser1)
plot(output)
```

Deseasonalize the data.  This shows the noise (`random`) + the `trend`
```{r}
t_des <- t_ser1 - output$seasonal
plot(t_des)
acf_des <- acf(as.vector(t_des), type = "correlation", lag.max = 64)
```
What we see above is a small coherence in the data, i.e. there is still a signal, but not very much.


What happens if we analyze a time series composed of random values:
```{r}
t_ser_random <- ts(runif(20*12), freq = 12)
output = decompose(t_ser_random)
plot(output)
```

The `decompose` method does show a seasonal compoment, but it's important to note the scale - the range of random values is much greater than the range of seasonal values

**TODO** use what's below with the temperature data above, show how this helps to evaluate the outcome.  Also can we set the y axes to the same scale? (use a separate plot for this, relative is useful)

```{r}
df1 = data.frame(random = output$random, 
                 seasonal = output$seasonal, 
                 trend = output$trend, 
                 observed = as.numeric(t_ser_random))

ggplot() + geom_density(data=df1, aes(x=random, fill="random"), alpha=0.5) + 
  geom_density(data=df1, aes(x=seasonal, fill="seasonal"), alpha=0.5) + 
  geom_density(data=df1, aes(x=trend, fill="trend"), alpha=0.5) + 
  geom_density(data=df1, aes(x=observed, fill="observed"), alpha=0.5) + xlab("value")
```

Look at the range (min and max values) for the random component and also the seasonal trend:
```{r}  
range(output$random, na.rm=TRUE)
range(output$seasonal, na.rm=TRUE)
```

### Exercise
Decompose the time series using classical decomposition. 

Hint: you need to construct an appropriate time series object first. What frequency do you choose for the time series object? Show the plot and give an interpretation of all elements of the plot. What is the unit of the x-axis?

**TODO** mention which data set

```{r, echo=FALSE}
series2.1 <- ts(series2$x, frequency = 7)
dec2 <-decompose(series2.1, type = "additive")
plot(dec2)
```


## Seasonal Decomposition of Time Series by Loess



**TODO** explain syntax

`stl`

```{r, eval=FALSE}
stl(x, s.window, 
    t.window = NULL)
```

Arguments:

* `x` - univariate time series to be decomposed. This should be an object of class "ts" with a frequency greater than one.
* `s.window` - either the character string "periodic" or the span (in lags) of the loess window for seasonal extraction, which should be odd and at least 7, according to Cleveland et al. This has no default.
* `t.window` - the span (in lags) of the loess window for trend extraction, which should be odd. If NULL, the default, nextodd(ceiling((1.5*period) / (1-(1.5/s.window)))), is taken.

**TODO** this is `stats::stl`, typing `?stl` will bring up the help mentioning that stl is also covered in the forecase package
**TODO** put all the contents of the practical ppt in the doc here for stl decomposition only.  Can leave out the other stuff

**TODO** mention the data set, could reload

```{r}
output1 = stl(t_ser1, s.window = 15, t.window = 12*15+1)
plot(output1)
```

Take the remainder and see the auto-correlation.  Could see some coherence, such as if one day was hot, the next day was likely to be hot as well.

If one month was warmer than average, the next month is likely to be warmer than average as well.  This is separate from the seasonal pattern.   

heat waves span the borders of the months

also head-start example with the sea - if warm May, then will have warm June, since additional heat is coming in (June should be warmer than May).  Have a higher starting point with the temperature.

```{r}
t_rem <- output1$time.series[,"remainder"]
acf_rem <-acf(as.vector(t_rem), type = "correlation", lag.max = 64)
```

On the right of the plot you will see several gray bars.  These give an indication of the scale of the different plots.  For example, the width of the bar on the `trend`  plot corresponds to the same range as the smaller gray bar on the `data` plot.  In other words, the trend is much smaller than the seasonal values.

### Exercise

**TODO** figure out if the exercises fit here.  Check that it's been covered how to do them.

Decompose the time series using `stl` decomposition. 

Play with the parameters `s.window` and `t.window` and observe how this influence the trend, the remainder, and the seasonal pattern.

**TODO** mention the data set

```{r, echo=FALSE}
dec2.1 <- stl(series2.1, s.window = 9, t.window = 13)
plot(dec2.1)
```

Calculate and plot the de-trended time series. 

Reminder for the decomposition produced by a <- stl(…).

```{r, echo=FALSE}
series2.1.des <-series2$x - dec2.1$time.series[,"trend"]
plot(series2.1.des)
```

Plot and interpret the ACF of the de-trended time series.

```{r, echo=FALSE}
acf(series2.1.des,type = "correlation", lag.max = 30)
```

With the help of the ACF investigate if the remainder still contains information, or if it is random noise. 

```{r, echo=FALSE}
acf(na.omit(dec2.1$time.series[,"remainder"]), type = "correlation", lag.max = 30)
```

there is an indication of a weak correlation at lag 1 and a weak anticorrelation at lag 2 and 3. This might indicate a causal relationship i.e., that a day with much internet traffic is followed by another day with much internet traffic. But since the correlation is quite weak is might also be due to imperfect decomposition or due to outliers (i.e. the first week with much higher than average weekly cycle. 
