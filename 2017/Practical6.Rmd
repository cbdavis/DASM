---
title: "DASM Practical 6"
author: "Chris Davis"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: false
    number_sections: false
---
  
# {.tabset .tabset-fade .tabset-pills}

## Basic linear fitting with and without weights 

### Getting Started

For this practical, we will need the `GGally` and the `lmodel2` libraries.  Make sure that they are both installed.  You only have to do this once.
```{r, eval=FALSE}
install.packages("GGally")
install.packages("lmodel2")
```

Now load required libraries.  
```{r, message=FALSE, warning=FALSE}
library(GGally)
library(lmodel2)
library(tidyverse)
```

### Reminder from the lecture

* We have data (x, y), where x is a deterministic variable and y is a random variable that is dependent on x. 
* We fit a linear regression line to the data with slope $\beta_1$ and intercept $\beta_0$ by finding the minimum of the RSS (Residual Sum of Squares)

$RSS(\beta_0, \beta_1) = \sum_{i=1}^{n} (y_{i} - \beta_0 - \beta_1 x_i)^2$

Where $\beta_1$ and $\beta_0$ are calculated using:

* $\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i-\bar{x})^2}$
* $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$

This equation requires that the variance in y is the same for all $x_i$

If this is not the case, then we apply weighted regression, by minimizing:

$RSS(\beta_0, \beta_1) = \sum_{i=1}^{n} w_i (y_{i} - \beta_0 - \beta_1 x_i)^2$

with $w_i = 1 / \sigma_i^2$

### Syntax

In R, the slope ($\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i-\bar{x})^2}$) and intercept ($\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$) can be calculated by these commands:

```{r, eval=FALSE}
b1 = cov(x,y)/var(x)
b0 = mean(y) â€“ b1*mean(x)
```

For the general linear regression, we use the `lm()` command.  The syntax you will need to use looks like:

```{r, eval=FALSE}
z = lm(formula, data, weights)
```

A summary of what you see when typing `?lm` into the console is:

* `formula` - a symbolic description of the model to be fitted
    * A typical model has the form `response ~ terms` where response is the (numeric) response vector and terms is a series of terms which specifies a linear predictor for response.
    * A terms specification of the form `first + second` indicates all the terms in first together with all the terms in second with duplicates removed.
* `data` - the data frame used as input.  The names you specify in `formula` will correspond to the column names in this data frame.
* `weights` - an optional vector of weights to be used in the fitting process.
   
### Example

#### Anscombe's Quartet

Throughout this practical, we'll use a data frame containing [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) as an example.  The data consists of four hypothetical collections of data that each have x and y values.  The data frame `anscombe` has eight columns which represent all four of these data sets.  For example, the first data set uses the columns `x1` and `y1`, the second uses `x2` and `y2`, etc.  You can type `?anscombe` into the console to find out more about this.

We'll first look at some basic statistical properties.  In the examples below, we use `c()` to group together the results into a vector so that they're easier to see all in a row instead of being printed out on separate lines.  

As you can see, the mean of x values for all the data sets are the same:
```{r}
c(mean(anscombe$x1), mean(anscombe$x2), mean(anscombe$x3), mean(anscombe$x4))
```

As is the variance of x values:
```{r}
c(var(anscombe$x1), var(anscombe$x2), var(anscombe$x3), var(anscombe$x4))
```

The mean of y values is the same to several decimal places:
```{r}
c(mean(anscombe$y1), mean(anscombe$y2), mean(anscombe$y3), mean(anscombe$y4))
```

The variance of y values is also very similar:
```{r}
c(var(anscombe$y1), var(anscombe$y2), var(anscombe$y3), var(anscombe$y4))
```

However, when we plot the different data sets, we see that they all look quite different:
```{r, echo=FALSE}
anscombe_2cols = data.frame(x = c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4),
                            y = c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4),
                            series = c(rep("Data 1", 11), rep("Data 2", 11), rep("Data 3", 11), rep("Data 4", 11)))

ggplot(anscombe_2cols, aes(x=x,y=y)) + geom_point() + facet_wrap(~series)
```

Anscombe's work has been taken a step further by a [recent paper](https://www.autodeskresearch.com/publications/samestats) that showed how to generate datasets that look different but have nearly identical statistical properties.

<center><img src="./images/DinoSequentialSmaller.gif"></center>

#### Perform a linear regression

For this example, we'll use the x and y values from the first data set in Anscombe's Quartet.  We'll create a second data frame to avoid overwriting the original data.

```{r}
anscombe1 = data.frame(x = anscombe$x1, 
                       y = anscombe$y1)
```

Now we perform a linear regression on the data:
```{r}
z = lm(y~x, data=anscombe1)
```

* `data=anscombe1` means that we want to use the `anscombe1` data frame that we just created
* `y~x` says that we're trying to predict a response (the value `y`) given the value of a term (`x`).  Here `x` and `y` correspond to the actual column names of the data frame.  If we were to us a different data frame (like `mtcars`) we would update the formula (for example to predict miles per gallon given horse power, we would use `mpg~hp`).

#### Basic regression output 
Using `summary` we can get an overview of the results of the regression
```{r}
summary(z)
```

#### Error bar plots in ggplot

For some data sets, you will also have information available on the error for particular values.  Anscombe's quartet doesn't contain this information, but for the sake of demonstrating the R syntax to create a plot with error bars, we just add a `y_error` column using random values and pretend that this is a real error measurement.

```{r}
# create a column with bogus error values so we can show how geom_errorbar() works
anscombe1$y_error = runif(nrow(anscombe1))
```

Now we perform a scatter plot using `geom_point`, but now we also add `geom_errorbar` to add error bars on top of those points.  The code `ymin = y - y_error` and `ymax = y + y_error` shows how we use the values from the `y` and `y_error` columns to calculate the top and bottom of the error bars.  

```{r}
ggplot(anscombe1, aes(x=x, y=y)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = y - y_error,
                    ymax = y + y_error,
                    width=0.3)) # width of the horizontal bar at the top of the error bar
```

In this example, the error bars are "mirrored" around the value of `y`.  Since we specify the value of `ymin` independently of `ymax`, then it's possible that you could use a different formula and/or column so that the observed value of `y` does not lie directly in the middle of the error bars but is skewed to one side.

In the example below, we create columns for `y_error_lower` and `y_error_upper`, in order to show how to create error bars where the error is not symmetrical around the data point.

```{r}
# Create columns with more bogus error values, 
# so we can show how geom_errorbar() works for more skewed error bars
# The values assigned here just make sure that the upper error bar 
# is further away from the data point than the lower error bar
anscombe1$y_error_upper = anscombe1$y + 3 * sd(anscombe1$y)/5
anscombe1$y_error_lower = anscombe1$y - sd(anscombe1$y)/5

ggplot(anscombe1, aes(x=x, y=y)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = y_error_lower,
                    ymax = y_error_upper,
                    width=0.3))
```


#### Add a regression line to the error bar plot 

We will reuse the same code from above, but will now add in a regression line.  To do this, we need to use the coefficients that were found from the linear regression.
```{r}
z$coefficients
```

As you can see, the first value is the intercept.  The second value is labelled `x` although this refers to the slope of the regression line.  We will use these values directly in the `geom_abline` command:

```{r}
ggplot(anscombe1, aes(x=x, y=y)) + geom_point() + 
  geom_errorbar(aes(ymin=y - y_error,
                    ymax=y + y_error,
                    width=0.3)) +             # same plotting code as above
  geom_abline(intercept = z$coefficients[1],  # but add in the regression line
              slope = z$coefficients[2])
```

Alternatively, we can also use `geom_smooth` to perform a linear regression, although for the examples in this practical, we'll use `geom_abline`.  Using `geom_smooth` is a good idea when you have to create a plot with multiple linear regressions that are done on subsets of the data.

```{r}
ggplot(anscombe1, aes(x=x, y=y)) + geom_point() + 
  geom_errorbar(aes(ymin=y - y_error,
                    ymax=y + y_error,
                    width=0.3)) + 
  geom_smooth(aes(x=x, y=y), method="lm", formula=y~x, se=FALSE)
```

One thing which we'll come back to later is that the linear regressions for the four data sets in Anscombe's Quartet are nearly identical, even though visually they look quite different:
```{r, echo=FALSE}
ggplot(anscombe_2cols, aes(x=x,y=y)) + 
  geom_point() + 
  geom_smooth(aes(x=x, y=y), 
               method="lm", formula=y~x, 
              se=FALSE, fullrange=TRUE) + 
  facet_wrap(~series)
```

### Exercise

For this exercise, download the data [Practical6_Data.txt](https://raw.githubusercontent.com/cbdavis/DASM/master/2017/data/Practical6_Data.txt) and load it into R.  Assume that the error is symmetrical around the data points.

#### Make an error bar plot

You should see:

```{r, echo=FALSE}
data1 <- read.csv("./data/Practical6_Data.txt")

ggplot(data1, aes(x=x_values, y_values)) + geom_point() + 
 geom_errorbar(aes(ymin=y_values - y_errors,
                   ymax=y_values + y_errors,
                   width=0.005)) # width changes the width of the horizontal bar at the top of the error bar
```

#### Perform a linear regression
Using `summary` on the results, you should see:

```{r, echo=FALSE}
##fit a "normal" fit
model5 <-lm(y_values~x_values, data1)
##abline(model5) 

##to get the information about the fit
summary(model5)
```

#### Add the regression line to the error bar plot
```{r, echo=FALSE}
ggplot(data1, aes(x=x_values, y_values)) + geom_point() + 
  geom_errorbar(aes(ymin=y_values - y_errors,
                    ymax=y_values + y_errors,
                    width=0.005)) + 
  geom_abline(intercept = model5$coefficients[1], 
              slope = model5$coefficients[2])
```

#### Perform a linear regression with weights specified

  For the linear regression, we need to tell `lm()` that the `weights` are `1/(data1$y_errors^2)`

The summary of the linear regression results should show:
```{r, echo=FALSE}
## take into account the error bars
model6 <-lm(y_values~x_values, data = data1, 
            weights = 1/(data1$y_errors^2))

## to get information about the fit
summary(model6)
```


#### Plot both linear regressions (with and without weights)
Remember that with ggplot, you can append commands like `+ geom_abline(..., color="blue") + geom_abline(..., color="red")`

```{r, echo=FALSE}
ggplot(data1, aes(x=x_values, y_values)) + geom_point() + 
 geom_errorbar(aes(ymin=y_values - y_errors,
                   ymax=y_values + y_errors,
                   width=0.005)) + # width changes the width of the horizontal bar at the top of the error bar
 geom_abline(aes(slope = model5$coefficients[2], intercept=model5$coefficients[1]), color="red") +
 geom_abline(aes(slope = model6$coefficients[2], intercept=model6$coefficients[1]), color="blue")
```

## Regression diagnostic

### Residuals ($\epsilon_i$)

#### Reminder from the lecture

* $E(\epsilon_i|X_i) = 0$ (residuals should have mean 0 for all $X_i$ )
* $\epsilon_i$ is normally distributed (residuals are normally distributed with mean = 0 and variance $Var(\epsilon_i) = s_2$, for all $\epsilon_i$

#### Syntax

For plotting residuals against fitted values, we'll again use the `anscombe1` data frame that we created above, using the same code for the linear regression:

```{r}
z = lm(y~x, anscombe1)
```

Now we get the fitted values corresponding to each value of x:
```{r}
fitted(z)
```

We then get the residuals:
```{r}
residuals(z)
```

Add in columns to the `anscombe1` data frame for the fit and res values
```{r}
anscombe1$fit = fitted(z)
anscombe1$res = residuals(z)
```

We can see that the fitted values follow the regression line:

```{r}
ggplot(anscombe1) + 
  geom_point(aes(x=x, y=y, color="original value"), size=3) + 
  geom_point(aes(x=x, y=fit, color="fitted value"), size=3) + 
  geom_abline(intercept = z$coefficients[1], slope=z$coefficients[2])
```

In the code above, we have the statements `color="original value"` and `color="fitted value"`.  When ggplot sees statements like these withing `aes(...)` it will automatically assign a color to each of these text values.  What is happening is the same as what you would get when using something like `aes(..., color=some_column_name)`.  In this case, every distinct value of `some_column_name` will be assigned a unique color.

Show the residuals that were calculated:
```{r}
ggplot(anscombe1) + 
  geom_point(aes(x=x, y=res))
```

Putting this all together we can see how subtracting the residual from the original `y` values gives us the fitted values in `fit`.  Note that `geom_segment` allows us to draw line segments from the points defined at `x`,`y` to the point at `xend`,`yend`

```{r}
ggplot(data = anscombe1) + 
  geom_point(aes(x=x, y=y, color="original value"), size=3) + 
  geom_point(aes(x=x, y=fit, color="fitted value"), size=3) + 
  geom_segment(aes(x=x, xend=x, y=y, yend=y-res), 
               arrow = arrow(length = unit(0.3,"cm"))) + # add an arrow to this line segment
  geom_abline(intercept = z$coefficients[1], slope=z$coefficients[2])
```

Plot residual against fitted values.  If our original data was from a straight line, then we would just see residual values of just zero.

```{r}
ggplot(anscombe1, aes(x=fit, y=res)) + geom_point() + ggtitle("residual vs. fitted values")
```

Perform a qqnorm plot of residuals to see if they are normally distributed:
```{r}
qqnorm(anscombe1$res)
qqline(anscombe1$res)
```

### Exercise

Make the plots of fitted values vs residuals for the data from `Practical6_Data.txt `

Perform a linear regression without weights:

```{r, echo=FALSE}
data1 <- read.csv("./data/Practical6_Data.txt")

model5 <-lm(y_values~x_values, data1)

##get the fitted values
fitted5 <- fitted(model5)
##get the residuals
res5 <- residuals(model5)

data1$res5 = res5
data1$fitted5 = fitted5

## plot residual against fitted values
ggplot(data1, aes(x=fitted5, y=res5)) + geom_point()
```

Make a qnorm plot of the residuals:

```{r, echo=FALSE}
qqnorm(res5)
qqline(res5)
```

Linear regression with weights (performed same as earlier in the practical):

```{r, echo=FALSE}
model6 <-lm(y_values~x_values, data = data1, 
            weights = 1/(data1$y_errors^2))

##get the fitted values
fitted6 <- fitted(model6)
##get the residuals
res6 <- residuals(model6)

data1$res6 = res6
data1$fitted6 = fitted6

## plot residual against fitted values
ggplot(data1, aes(x=fitted6, y=res6)) + geom_point()
```

Make a qnorm plot of the residuals (using the weighted model):
```{r, echo=FALSE}
qqnorm(res6)
qqline(res6)
```


## Cook's distance

### Reminder from the lecture

The Cookâ€™s distance of point $X_i$ is a measure of the difference in regression parameters when the point is included or omitted. If omitting a point changes the regression parameters strongly, this point has a large Cookâ€™s distance.

To see quantitatively if the Cook's distance of a data point is too large and it has an undue influence on the fit you need to compare it with a theoretical cut-off.  

This cutoff value can be calculated with the F-statistic using `qf(0.5, p, n-p, lower.tail = FALSE)` where:

* `p` - fitted parameters (=2 with simple linear regression)
* `n` - number of data points 


### Syntax

Below we use the `anscombe1` data frame again.

```{r}
##Cook's distance

z = lm(y~x, data=anscombe1)
cook <- cooks.distance(z)

## put the fitted values and Cook's distance into a data frame
data = data.frame(fit = fitted(z),
                  cooks_distance = cook)

##cut-off value
cut <- qf(0.5, 2, length(data$fit)-2, lower.tail = FALSE)

ggplot(data, aes(x=fit, y=cooks_distance)) + 
  geom_point() +                   # plot fitted values vs. cooks distance for each fitted values
  geom_hline(yintercept=cut)       # add a horizontal line for the cut-off we calculated
```

If we look at all four data sets in Anscombe's Quartet, there are quite different outcomes when plotting the Cook's distance for each.

```{r, echo=FALSE, warning=FALSE}
cook1 = cooks.distance(lm(y~x, anscombe_2cols %>% filter(series == "Data 1")))
cook2 = cooks.distance(lm(y~x, anscombe_2cols %>% filter(series == "Data 2")))
cook3 = cooks.distance(lm(y~x, anscombe_2cols %>% filter(series == "Data 3")))
cook4 = cooks.distance(lm(y~x, anscombe_2cols %>% filter(series == "Data 4")))

cut <- qf(0.5, 2, 11-2)

anscombe_2cols$cooks_distance = c(cook1, cook2, cook3, cook4)

ggplot(anscombe_2cols, aes(x=x, y=cooks_distance)) + 
  geom_point() + 
  facet_wrap(~series) + geom_hline(yintercept = cut)
```

### Exercise

For the unweighted and weighted fit you performed on `Practical6_Data.txt`, compare the Cook's distance.

Unweighted fit:

```{r, echo=FALSE}
dat = data.frame(fit = fitted(model5), 
                 cooks_distance = cooks.distance(model5))

cut5 <- qf(0.5, 2, length(dat$fit)-2, lower.tail = FALSE)

ggplot(dat, aes(x=fit, y=cooks_distance)) + 
  geom_point() + geom_hline(yintercept = cut5)
```

Weighted fit:

```{r, echo=FALSE}
##Cooks distance
dat = data.frame(fit = fitted(model6), 
                 cooks_distance = cooks.distance(model6))
cut6 <- qf(0.5, 2, length(dat$fit)-2, lower.tail = FALSE)
ggplot(dat, aes(x=fit, y=cooks_distance)) + 
  geom_point() + geom_hline(yintercept = cut6)

```

## Automatic diagnostic plots

We can create a series of diagnostic plots for the linear regression results by simply using `plot(z)` (where `z` is the output of a linear regression) instead of `ggplot`.  What's happening here is that R detects that `z` is the result of a linear regression, and whenever it sees this, it will create a series of four plots: 

* Residuals vs. Fitted
* Normal Q-Q
* Scale-Location (not covered in this practical)
* Residuals vs. Leverage (related to Cook's distance)

Here we create the diagnostic plots for the linear regression of the `anscombe1` data frame:
```{r}
z = lm(y~x, data=anscombe1)
plot(z)
```

### Exercise

Create diagnostic plots for both the unweighted and weighted regressions that you just did on the data for from `Practical6_Data.txt`.

#### Diagnostic plots for unweighted fit
```{r, echo=FALSE}
plot(model5)
```

#### Diagnostic plots for weighted fit
```{r, echo=FALSE}
plot(model6)
```


## Multiple regression

### Reminder from the lecture

$Y_i =  \beta_0 + \beta_1*X_{1i} + \beta_2*X_{2i} +  \beta_3*X_{3i} + â€¦ +  \epsilon_i$

Interpretation of the parameters:

* If $\beta_j  > 0$, then $j$ stands for the average increase of the response when predictor $x_j$ increases with one unit, holding all other variables constant.
* If $\beta_j  < 0$, then $j$ stands for the average decrease of the response when predictor $x_j$ increases with one unit, holding all other variables constant.

An ANOVA test can be done to see if the removing one variable significantly changes the fit. If p-value of the ANOVA test <0.05, you should keep the variable (as a rule of thumb)
 
### Syntax 

#### Linear model as a function of several parameters
 
If you have several variables (y, x1, x2, x3, â€¦) in the data frame `dat`, you can perform multiple regression via this syntax:

```{r, eval=FALSE}
z = lm(y~ x1 + x2 + x3 + â€¦, data = dat) 
```

#### A pairs plot

A pairs plot is useful for doing an initial exploration of how different variables (i.e. columns in a data frame) may be related to each other.  A pairs plot is presented as a matrix containing scatter plots of all the combinations of variables with each other.

To be able to do this, make sure to install and load the `GGally` library as described above.

```{r, eval=FALSE}
install.packages("GGally")
library(GGally)
```

As an example, we'll use a pairs plot on the `mtcars` dataset.  For this, we only use the `mpg`, `disp`, `hp` and `wt` columns.  Looking at the first few rows in these columns, we see:

```{r}
head(mtcars[,c("mpg", "disp", "hp", "wt")])
```

The pairs plot for these four columns looks like this:
```{r}
ggpairs(mtcars[,c("mpg", "disp", "hp", "wt")])
```

Reading across the rows and columns we can see which variables are being plotted together.  In the first column and second row, we have the `disp` vs. the `mpg`, directly below that is the `hp` vs. the `mpg` and so on.  On the top right we can see the correlations calculated between the variables.  On the diagonal we see a density plot of the variables.  For example, most of the values for `hp` are around 100, although the distribution is skewed to one side.

Now we perform a linear regression where we try to predict `mpg` given the values for `disp`, `hp` and `wt`

```{r}
z = lm(mpg~disp + hp + wt, data=mtcars)
summary(z)
```

We can also extract the $R^2$ value from the summary:
```{r}
summary_for_z <- summary(z)
print(summary_for_z$r.squared)
```

Create a series of diagnostic plots:

```{r}
plot(z)
```

#### ANOVA test

We can perform an ANOVA test using the following set up (assuming a data frame named `dat` with columns named `y`, `x1`, `x2`, and `x3`)

* `z1 = lm(y~ x1 + x2 + x3, data = dat)`
* `z2 = lm(y~ x1 + x2, data = dat)`
* `anova(z1,z2)`

Using the `mtcars` example, we can do:
```{r}
z1 = lm(mpg~disp + hp + wt, data=mtcars)
z2 = lm(mpg~disp + hp, data=mtcars)  # leave out `wt`
results = anova(z1, z2)
print(results)
```

As stated above, if p-value of the anova test <0.05, you should keep the variable (as a rule of thumb).  Here it's clear that the `wt` column (weight) is quite important in predicting the fuel economy of a car.

### Exercise

For this we'll use the `stackloss` data set with was mentioned in Lecture 8.  This data frame is already pre-loaded with R, and you can get more details about it by typing `?stackloss` in the console.

#### Look at a summary of the data set
```{r, echo=FALSE}
summary(stackloss)
```

#### Make a pairs plot

```{r, echo=FALSE}
ggpairs(stackloss)
```

#### Fit a linear model using all variables and summarize the output of the model
For this we are trying to predict `stack.loss` given all the other variables

```{r, echo=FALSE}
mul_fit1 <- lm(stack.loss ~ Acid.Conc. + Air.Flow + Water.Temp, data = stackloss)

summary(mul_fit1)
```

#### Plot the standard diagnostic plots

```{r, echo=FALSE}
plot(mul_fit1)
```

#### Make a Cook's distance plot by hand with a cutoff line

```{r, echo=FALSE}
dat = data.frame(fit = fitted(mul_fit1),
                 cooks_distance = cooks.distance(mul_fit1))

p = 4
n = nrow(stackloss)
cut <- qf(0.5, p, n - p, lower.tail=FALSE)

ggplot(dat, aes(x=fit, y=cooks_distance)) + geom_point() + geom_hline(yintercept=cut)
``` 

#### Leave out the least important parameter Acid concentration
```{r, echo=FALSE}
less_fit<- lm(stack.loss~Air.Flow+Water.Temp, data = stackloss)
summary(less_fit)
```

#### See if that made a difference for the overall model
```{r, echo=FALSE}
anova(mul_fit1,less_fit)
```

#### Now leave out water temp
```{r, echo=FALSE}
least_fit<- lm(stack.loss~Air.Flow, data = stackloss)
summary(least_fit)
```

Perform the anova test:
```{r, echo=FALSE}
anova(less_fit, least_fit)
```

#### Fits of individual varaibles

For this create a set of linear regressions for combinations of the individual variables

##### Regression for `stack.loss~Air.Flow`

Value for $R^{2}$:
```{r, echo=FALSE}
fit_air <- lm(stack.loss~Air.Flow, data = stackloss)
sum_air <- summary(fit_air)
R2_air <-sum_air$r.squared
print(R2_air)

ggplot(stackloss, aes(x=Air.Flow, y=stack.loss)) + geom_point() + 
  geom_abline(intercept = fit_air$coefficients[1], slope=fit_air$coefficients[2])
```

##### Regression for `stack.loss~Water.Temp`
Value for $R^{2}$:
```{r, echo=FALSE}
fit_temp <- lm(stack.loss~Water.Temp, data = stackloss)
sum_temp <- summary(fit_temp)
R2_temp <-sum_temp$r.squared
print(R2_temp)

ggplot(stackloss, aes(x=Water.Temp, y=stack.loss)) + geom_point() + 
  geom_abline(intercept = fit_temp$coefficients[1], slope=fit_temp$coefficients[2])

```

##### Regression for `Air.Flow~Water.Temp`
Value for $R^{2}$:
```{r, echo=FALSE}
fit_ind <- lm(Air.Flow~Water.Temp, data = stackloss)
sum_ind <- summary(fit_ind)
R2_ind <-sum_ind$r.squared
print(R2_ind)

ggplot(stackloss, aes(x=Water.Temp, y=Air.Flow)) + geom_point() + 
  geom_abline(intercept = fit_ind$coefficients[1], slope=fit_ind$coefficients[2])

```

## Replacing values with NA 

Real data may often have missing values, which are represented in R as `NA`.  In some data sets you may see them represented in other forms like `NaN` (not a number) or by an unrealistic number (e.g., -999).  For R to handle the missing values, you have to rename them as `NA`, if they are not named like that, then R does not recognize them and will try to process them as an actual number.

Let's say that for a data frame we have called `df`, `NA` values are currently represented as `-999`.  If we type into the console `df == -999`, we'll get a matrix of `TRUE`, `FALSE` or `NA` values (if `NA` is already present).  

To replace these -999 values, we use the command below which says that for all locations in the data frame with a value of -999, we should assign to those locations the value `NA` instead.

```{r, eval=FALSE}	
df[df == -999] <-NA
```

Also be aware that many R functions have arguments that allow you to specify how they will process `NA` values.
For example, if you type `?mean` in the R console, you'll see that the `mean()` function has a `na.rm` argument that specifies if `NA` values should be removed before computing the mean.

Calculating `mean()` with `NA` values included:
```{r}
mean(c(3,7,3,NA,5))
```

Calculating `mean()` and removing `NA` values:
```{r}
mean(c(3,7,3,NA,5), na.rm=TRUE)
```

The `lm()` function has a `na.action` argument that specifies how to deal with missing values.
```{r, error=TRUE}
df <- data.frame(x = c(1, 2, 3), y = c(0, 10, NA))

# remove rows from the data frame containing NA
na.omit(df)

lm(y~x, df, na.action=na.fail)
lm(y~x, df, na.action=na.omit)
lm(y~x, df, na.action=na.pass)
```

The `dplyr` library also deals with `NA` values in a specific way:

* If you are doing a numeric comparisons (i.e. `hp >= 250`) using the `filter` function, then `NA` values will be filtered out.  
* If you want to do a numeric comparison and also keep `NA` values, then you need to specify something like `hp >= 250 | is.na(hp)`.  The function `is.na()` will return `TRUE` for any `NA` values.


### Exercise

For this exercise, download the [Carbon.csv](https://raw.githubusercontent.com/cbdavis/DASM/master/2017/data/Carbon.csv) data into R as a data frame called `input1`.

```{r, echo=FALSE, cache=TRUE}
input1 = read.csv("https://raw.githubusercontent.com/cbdavis/DASM/master/2017/data/Carbon.csv")
```

After loading in the csv file into R, try to replace the -999 values with `NA`.  If you run `summary` before the replacement you should see:

```{r, echo=FALSE, cache=TRUE}
summary(input1)
```

After replacing them, using `summary` you should see that several columns now have `NA` values:
```{r, echo=FALSE, cache=TRUE}
input1[input1 == -999] <-NA
summary(input1)
```
