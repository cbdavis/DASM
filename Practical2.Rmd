---
title: "DASM Practical 2"
author: "Chris Davis"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE, message=FALSE}
library(animation)
```


Should cover binomial distribution - this is one of the homework problems.  
"Common distributions" - Example 1 - look at cars @ intersection example.  Show what to put into R

Ask them to do example 2 & 3 in R, based on what I showed them with example 1.
For example 1, plot the distribution, would check to see if answers are reasonable or not.  

Take out combinatorics, permutations, leave in Bayes example with cows.

---

# Background

R contains functions that allow you to easily work with distributions.  The names of these functions follow a standard format - you'll see a "d", "r", "p" or "q" and then the name of the distribution.

The "d", "r", "p" or "q" stands for:

* **d** - density functon for distribution
* **r** - random sample from distribution
* **p** - cumulative distribution
* **q** - quantile function

Below you can see how these letters are combined with the names of the distributions:

<center>
<table width=400><tr><td>
| Distribution<br>Name | Random<br>Samples  | Density<br>Function | Cumulative<br>Distribution | Quantile |
|----------------------+--------------------+---------------------+----------------------------+----------|
| Normal               | `rnorm`            | `dnorm`             | `pnorm`                    | `qnorm`  |
| Poison               | `rpois`            | `dpois`             | `ppois`                    | `qpois`  |
| Binomial             | `rbinom`           | `dbinom`            | `pbinom`                   | `qbinom` |
| Uniform              | `runif`            | `dunif`             | `punif`                    | `qunif`  |
| Student t            | `rt`               | `dt`                | `pt`                       | `qt`     |
| Chi-Squared          | `rchisq`           | `dchisq`            | `pchisq`                   | `qchisq` |
| F                    | `rf`               | `df`                | `pf`                       | `qf`     |
</td></tr></table>
*Source: [Base R Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2016/06/r-cheat-sheet.pdf)*
</center>

Type `?Distributions` into the RStudio Console in order to see documentation on all of the distributions

# Exercises

Load required library and set `stringsAsFactors = FALSE`
```{r}
library(ggplot2)
options(stringsAsFactors = FALSE)
```

## Plot one or two of these distributions and their cumulative distributions 
x vs f(x) 

```{r}
# create a series of x values
xvals = seq(-3, 3, 0.01)

# create a data frame containing values for the density function (sampled at
# each value of xvals) and the cumulative distribution sampled at the same values

dist_data = data.frame(x=xvals, 
                       dist = dnorm(xvals),
                       cumulative_dist = pnorm(xvals))

ggplot() + 
  geom_line(data=dist_data, aes(x=x, y=dist)) + 
  geom_line(data=dist_data, aes(x=x, y=cumulative_dist)) + 
  ylab("Probability") + 
  xlab("X Values")
```

## Find the probabilities 
that x > x0 or x < x0 (with the option lower.tail â€“ FALSE)
```{r}

x0 = 1
# x > x0
pnorm(x0, lower.tail=FALSE)
# x < x0
1 - pnorm(x0, lower.tail=FALSE)

```

## Use the qxxx distributions to find quartiles for different probabilities (also use upper and lower tails) 

```{r}
qnorm(0.80)
qnorm(0.90)
qnorm(0.95)
qnorm(0.99)
```

```{r}
ggplot() + 
  geom_line(data=dist_data, aes(x=x, y=dist), colour="blue") + 
  geom_line(data=dist_data, aes(x=x, y=cumulative_dist), color="red") + 
  ylab("Probability") + 
  xlab("X Values") + 
  geom_vline(xintercept = qnorm(0.5), linetype="dashed") + 
  geom_vline(xintercept = qnorm(0.9), linetype="dashed") +
  geom_vline(xintercept = qnorm(0.95), linetype="dashed") + 
  geom_vline(xintercept = qnorm(0.99), linetype="dashed") + 
  ggtitle("Quantitles for normal distribution at 0.5, 0.9, 0.95, 0.99")
```

## Take random values from a distribution 

```{r}
runif(10)
rnorm(10)
```

Keep this?
```{r animatedHistogram, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
x = seq(-3,3,0.01)
density = dnorm(x)
density_df = data.frame(x=x, 
                        y=density)

point_steps = c()
for (i in c(1:5)){
  point_steps = c(point_steps, c(1:10)*10^i)
}

saveGIF({  
  for (number_of_points in point_steps){
    
    set.seed(12345)
    df = data.frame(vals = rnorm(number_of_points))
    
    p = ggplot() + 
      geom_histogram(data=df, aes(x=vals, y = ..density..), bins=100) + 
      geom_line(data=density_df, aes(x=x, y=density)) + 
      coord_cartesian(xlim=c(-3,3), ylim=c(0,0.45)) + 
      xlab("Values") + ylab("Density") + ggtitle(paste(format(number_of_points, scientific=FALSE), "samples"))
    
    print(p)
  }
}, interval = 0.1, movie.name = "SampleDistributions.gif", ani.width = 400, ani.height = 300)

```

![Density versus samples](SampleDistributions.gif) 


## Explore an option to selected x random values y times 

[Q-Q plots](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot)

(if not done in the previous practicum: let them calculate normal qq plots to explore if a distribution is normal) 

**TODO:** See `help(package="datasets")` for other useful datasets

```{r}
x <- rnorm(100)
qqnorm(x)
qqline(x)
```

Uniform distribution

```{r}
x <- runif(100)
qqnorm(x)
qqline(x)
```

### Binomial distribution

Type `?rbinom` into the console will bring up a page showing the following parameters that need to be specified to generate random numbers from a binonial distribution:

* `n`: number of observations
* `size`: number of trials
* `prob`: probability of success on each trial

In the code below, `n=1000` means that we have 100 observations.  During each observation we do 50 trials (`size=50`) where the probability of a success is defined by `prob=0.5`.

```{r}
x <- rbinom(n=1000, size=50, prob=0.5)
```

We can use `range` to see the minimum and maximum values:
```{r}
range(x)
```

In other words, there exists at least one observation where there were `r min(x)` successes and at least one observation where there were `r max(x)` successes.

We use `+ xlim(c(0,50))` to show the x axis for values from 0 to 50.

```{r}
# Create a data frame for the values of x
# This results in a data frame with one column: data$x
data = as.data.frame(x)
ggplot(data, aes(x=x)) +  geom_histogram(binwidth = 1) + xlim(c(0,50))
```

```{r}
qqnorm(x)
qqline(x)
```

If we try a more skewed distribution with `prob=0.9`, we get:
```{r}
x <- rbinom(n=1000, size=50, prob=0.9)
data = as.data.frame(x)
ggplot(data, aes(x=x)) +  geom_histogram(binwidth = 1) + xlim(c(0,50))

qqnorm(x)
qqline(x)
```

We next use the `faithful` dataset which is included with R.  You can type `?faithful` in the RStudio console to get more information on it.  This data is about the [Old Faithful geyser](https://en.wikipedia.org/wiki/Old_Faithful) in Yellowstone National Park in the US.  This geyser is famous for erupting on a very regular schedule and the data set has information on how long the eruptions are (`faithful$eruptions`) and the amount of time until the next eruption (`faithful$waiting`).

We can first make a Q-Q plot of the waiting times:
```{r}
x = faithful$waiting
qqnorm(x)
qqline(x)
```

This tells us that the eruptions are clearly not normally distributed.  To investigate further, we can plot a histogram of the values:
```{r}
ggplot(faithful, aes(x=waiting)) + geom_histogram(binwidth=2)
```

From the histogram, we see from this is that it's clearly bimodal as there are two distinct peaks.

To investigate further, we can do a scatter plot showing how the waiting time might be related to the length of the eruption.
```{r}
ggplot(faithful, aes(x=eruptions, y=waiting)) + geom_point() + 
  xlab("length of eruption (minutes)") + 
  ylab("waiting time until next eruption (minutes)")
```

This shows us that the longer the eruption, the longer we will have to wait until the next one.  There also seem to be two distinct clusters.  It's not clear what is causing this, and since the data doesn't mention the date of the eruption, we don't know it randomly switches between short and long eruptions, or if for years there were long eruptions, but now there are only short eruptions due to factors such as earthquakes changing the water supply to the geyser.

We can also split up the data into two sets, where one lists all the eruptions that lasted less than three minutes, and the other one contains those which are longer:
```{r}
faithful_short = faithful[which(faithful$eruptions < 3),]
faithful_long = faithful[which(faithful$eruptions >= 3),]
```

Q-Q plot for the short eruptions:
```{r}
qqnorm(faithful_short$waiting)
qqline(faithful_short$waiting)
```

Q-Q plot for the long eruptions:
```{r}
qqnorm(faithful_long$waiting)
qqline(faithful_long$waiting)
```

This shows that if we split the data into two clusters, then data in those clusters seems to be normally distributed.

## Let then use the quartile functions to calculate confidence intervals  i.e. give them a data set of values (n<30, but from a normal distribution). 

**TODO** let students repeat this with a new dataset

* let them calculate a qq-normal plot to check if data are normal (**TODO**: done above?)
* Let them calculate confidence intervals for mean and standard deviation. (For this you can put the formula from the lecture at the slides.)

# Big Exercise

* Let them take a random sample n< 30 from a normal distribution
```{r}
n = 30 # number of random samples

# set the mean and standard deviation of the distribution
true_mean = 0
true_sd = 1

# take random samples
vals = rnorm(n, mean=true_mean, sd=true_sd)
```

Now we calculate $\mu$ and $\sigma$ of the distribution and compare it to to the original distribution (where $\mu = 0$ and $\sigma = 1$).

```{r}
mean_pts = mean(vals)
mean_pts
sd_pts = sd(vals)
sd_pts
```

From the book, we know that we can calculate the 95% confidence interval using the following formula:

$$\bar{y} \pm 1.96 \cdot \sigma_{\bar{y}} $$

Where:

$$\sigma_{\bar{y}} = \frac{\sigma}{\sqrt{n}}$$

* Calculate the 95% confidence interval of the mean and compare to the original distribution:
```{r}
```

For how many samples does the confidence interval include the true mean?   Here, for 500 iterations we take `n` random numbers from the distribution, and calculate the confidence intervals.

```{r}

times_true_mean_in_confidence_interval = 0
num_tests = 500
# create a loop where the value of "count" will be 1, 2, 3... num_tests
for (count in c(1:num_tests)){
  
  # sample random values from the distribution
  vals = rnorm(n, mean=0, sd=1)
  
  # calculate mean and standard deviation
  mean_pts = mean(vals)
  sd_pts = sd(vals)
  
  # calculate left and right sides of the interval
  left = mean_pts - (1.96 * sd_pts/sqrt(n))
  right = mean_pts + (1.96 * sd_pts/sqrt(n))
  
  # see if the true_mean is within the bounds of the interval
  if (true_mean >= left & true_mean <= right){
    # if it's in the interval, add one to this variable
    times_true_mean_in_confidence_interval = times_true_mean_in_confidence_interval + 1
  }
}

# show how many times the true_mean falls within the intervals calculated
times_true_mean_in_confidence_interval

# what percentage of the time does the true_mean fall within the intervals calculated?
times_true_mean_in_confidence_interval / num_tests
```

What if we ran this 10,000 times?  How would this percentage change as we repeatly took more random samples?

Note that the x axis is scaled logarithmically.

```{r, echo=FALSE}
times_true_mean_in_confidence_interval = 0

set.seed(1234)

num_tests = 10000
percent_times_true_mean_in_interval = c()
# create a loop where the value of "count" will be 1, 2, 3... num_tests
for (count in c(1:num_tests)){
  
  # sample random values from the distribution
  vals = rnorm(n, mean=0, sd=1)
  
  # calculate mean and standard deviation
  mean_pts = mean(vals)
  sd_pts = sd(vals)
  
  # calculate left and right sides of the interval
  left = mean_pts - (1.96 * sd_pts/sqrt(n))
  right = mean_pts + (1.96 * sd_pts/sqrt(n))
  
  # see if the true_mean is within the bounds of the interval
  if (true_mean >= left & true_mean <= right){
    # if it's in the interval, add one to this variable
    times_true_mean_in_confidence_interval = times_true_mean_in_confidence_interval + 1
  }
  
  percent_times_true_mean_in_interval = c(percent_times_true_mean_in_interval,
                                          (times_true_mean_in_confidence_interval/count))
}

data = data.frame(percent = percent_times_true_mean_in_interval,
                  iteration = c(1:num_tests))
ggplot(data, aes(x=iteration, y=percent)) + geom_point() + geom_line() +  
  xlab("Iteration") + 
  ylab("Percent times true mean in 95% confidence interval") + scale_x_log10()
```

This shows that there is a lot of variation at first, but as expected, with more samples, we settle into the 95% range.

# Bayes Theorem

$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$ 

* Event A: cow has BSE, $P(A)$=0.01
* Event B: test is positive
* $P(B|A)$ = 0.9 accuracy (= test is positive, if the cow is infected)
* $P(B|A^{c})$ = 0.1 false positives (= test is positive, if the cow is not infected)

What is the chance that B is positive on a randomly chosen cow?
0.01*0.9 + 0.99*0.1 = 0.108

This gray box represents 100% of the population of cows:

```{r BayesStep1, fig.width=4, fig.height=2.5, echo=FALSE}
ggplot() + 
  geom_rect(data=data.frame(xmin=0, ymin=0, xmax=1, ymax=1), 
            aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), fill="gray") + 
  theme_void() + 
  geom_text(aes(x = 0.5, y = 0.5, label = "All Cows"))
```

We know that 1% of the cow population has BSE:

```{r BayesStep2, fig.width=4, fig.height=2.5, echo=FALSE}
offset_BSE = sqrt(0.01)

xmin_BSE = 0.5 - offset_BSE/2
ymin_BSE = 0.5 - offset_BSE/2
# cover up 0.08964

ggplot() + 
  geom_rect(data=data.frame(xmin=0, ymin=0, xmax=1, ymax=1), 
            aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), fill="gray") + 
  geom_rect(data=data.frame(xmin=xmin_BSE, ymin=ymin_BSE, 
                            xmax=xmin_BSE + offset_BSE, 
                            ymax=ymin_BSE + offset_BSE), 
            aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), color="#0000FF77", fill="#0000FF77") + 
  theme_void() + 
  geom_text(aes(x = 0.5, y = 0.75, label = "All Cows")) +
  geom_text(aes(x = xmin_BSE + offset_BSE/2, y = ymin_BSE + offset_BSE/2, label = "BSE"))
```

But we also know that there are a lot of positive results:

```{r BayesStep3, fig.width=4, fig.height=2.5, echo=FALSE}
offset_Positive = sqrt(0.108)

xmin_Positive = 0.5 - offset_Positive/2
ymin_Positive = 0.5 - offset_Positive/2

ggplot() + 
  geom_rect(data=data.frame(xmin=0, ymin=0, xmax=1, ymax=1), 
            aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), fill="gray") + 
  geom_rect(data=data.frame(xmin=xmin_Positive, ymin=ymin_Positive, 
                            xmax=xmin_Positive + offset_Positive, 
                            ymax=ymin_Positive + offset_Positive), 
            aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), color="#FF000077", fill="#FF000077") + theme_void() + 
  geom_text(aes(x = 0.5, y = 0.75, label = "All Cows")) +
  geom_text(aes(x = 0.5, y = 0.5, label = "Positive Tests"))
```

Just from this, we can see that there are way more positive test results than the number of cows which have BSE, and therefore, getting a positive result may not be a good indication that the cow actually has BSE.

Combining everything together, we get the image shown below.  As you can see, if a cow actually has BSE, then there is a high probability that the test will spot it.  However, since most cows are not infected (99%) and we know that there is a ten percent chance of false positives, this results in a large part of the population that gets positive test results, but actually doesn't have BSE.

A 10% false positive rate (tests positive, no BSE) is actually quite high, when the size of the true positive (tests positive, has BSE) population is much smaller than the false positive rate.

```{r BayesStep4, fig.width=4, fig.height=2.5, echo=FALSE}
offset_BSE = sqrt(0.01)
offset_Positive = sqrt(0.108)

xmin_Positive = 0.5 - offset_Positive/2
ymin_Positive = 0.5 - offset_Positive/2

xmin_BSE = xmin_Positive - (0.1-0.08964)
#ymin_BSE = 0.5 - offset_BSE/2
ymin_BSE = ymin_Positive
# cover up 0.08964

ggplot() + 
  geom_rect(data=data.frame(xmin=0, ymin=0, xmax=1, ymax=1), 
            aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), fill="gray") + 
  geom_rect(data=data.frame(xmin=xmin_BSE, ymin=ymin_BSE, 
                            xmax=xmin_BSE + offset_BSE, 
                            ymax=ymin_BSE + offset_BSE), 
            aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), color="#0000FF77", fill="#0000FF77") + 
  geom_rect(data=data.frame(xmin=xmin_Positive, ymin=ymin_Positive, 
                            xmax=xmin_Positive + offset_Positive, 
                            ymax=ymin_Positive + offset_Positive), 
            aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), color="#FF000077", fill="#FF000077") + theme_void() + 
  geom_text(aes(x = 0.5, y = 0.75, label = "All Cows")) +
  geom_text(aes(x = 0.5, y = 0.5, label = "Positive Tests")) + 
  geom_text(aes(x = xmin_BSE + offset_BSE/2, y = ymin_BSE + offset_BSE/2, label = "BSE"))
```

What if we had different values for the probability that the test was accurate (i.e. different values for $P(B|A)$ and $P(B|A^{c})$)?

We still have the same probability of a cow having BSE, so we keep $P(A)$=0.01.

We need to calculate again $P(B)$ which is the probabilitiy of getting a positive result independent of whether the cow has BSE or not.

In R, we represent $P(B|A)$ as the variable `p_B_given_A` and we make a sequence from 0 to 100, which indicates a range of values for the test being 0% accurrate all the way up to 100% accurate at spotting BSE given that the cow actually has BSE.

$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$ 


```{r}
# probability has BSE
p_A = 0.01

# probability tests positive given has BSE (i.e. how accurate the test is if you have BSE)
p_B_given_A = seq(0.8,1,0.005)

# probability that the test is positive (independent of if you have BSE or not)
p_B = (p_A * p_B_given_A) +  # probability that the test is positive if you have BSE
  ((1-p_A) * (1-p_B_given_A))  # probability that the test is positive if you don't have BSE

df = data.frame(p_A = p_A,
                p_B_given_A = p_B_given_A,
                p_B = p_B)

df$p_A_given_B = (df$p_B_given_A * df$p_A) / df$p_B

head(df)

ggplot(df, aes(x=p_B_given_A, y=p_A_given_B)) + 
  geom_point() + 
  xlab("P(B|A) = P(Positive Test Results|Has BSE)
       If the cow has BSE, probability of spotting it with the test") + 
  ylab("P(A|B) = P(Has BSE|Positive Test Results)
       If cow has positive test results, probability that it actually has BSE")

```

There are two interesting things that we see here: 

* Past a certain point, as the test becomes more accurate in spotting cows with BSE, the number of false positives drop at a higher rate than the corresponding increase in accuracy.
* If BSE spreads, then your false positives go down, and the "accuracy" of the test (i.e. $P(B|A)$) goes up without actually changing anything with the test.  In other words, "accuracy" is partly a function of population size.